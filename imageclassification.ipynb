{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca8768c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:51:54.357155Z",
     "iopub.status.busy": "2025-04-30T09:51:54.356912Z",
     "iopub.status.idle": "2025-04-30T09:52:06.013095Z",
     "shell.execute_reply": "2025-04-30T09:52:06.012520Z"
    },
    "papermill": {
     "duration": 11.662902,
     "end_time": "2025-04-30T09:52:06.014469",
     "exception": false,
     "start_time": "2025-04-30T09:51:54.351567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler, Subset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n",
    "from collections import Counter\n",
    "\n",
    "# For reproducibility\n",
    "def set_seed(seed, use_gpu=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 123\n",
    "USE_SEED = True\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb9a5b",
   "metadata": {
    "papermill": {
     "duration": 0.003034,
     "end_time": "2025-04-30T09:52:06.021218",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.018184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212e19d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.028951Z",
     "iopub.status.busy": "2025-04-30T09:52:06.028150Z",
     "iopub.status.idle": "2025-04-30T09:52:06.036243Z",
     "shell.execute_reply": "2025-04-30T09:52:06.035599Z"
    },
    "papermill": {
     "duration": 0.012826,
     "end_time": "2025-04-30T09:52:06.037260",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.024434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root: str, test: bool = False, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.test = test\n",
    "\n",
    "        self.img_path = osp.join(root, 'images')\n",
    "        self.targets = []\n",
    "        self.ids = []\n",
    "\n",
    "        if not test:\n",
    "            # Load images and labels\n",
    "            labels_path = osp.join(root, 'labels.csv')\n",
    "            with open(labels_path, 'r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    image_id = row['id'].zfill(5)\n",
    "                    label = int(row['label'])\n",
    "                    self.targets.append(label)\n",
    "                    self.ids.append(image_id)\n",
    "        else:\n",
    "            # Test mode: no labels.csv\n",
    "            for fname in sorted(os.listdir(self.img_path)):\n",
    "                if fname.endswith('.jpeg'):\n",
    "                    image_id = fname[:-5].zfill(5)\n",
    "                    self.ids.append(image_id)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_id = self.ids[index]\n",
    "        img_file = osp.join(self.img_path, f'{img_id}.jpeg')\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.test:\n",
    "            return img, img_id\n",
    "        else:\n",
    "            target = self.targets[index]\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c4a45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.044155Z",
     "iopub.status.busy": "2025-04-30T09:52:06.043955Z",
     "iopub.status.idle": "2025-04-30T09:52:06.182544Z",
     "shell.execute_reply": "2025-04-30T09:52:06.181647Z"
    },
    "papermill": {
     "duration": 0.143268,
     "end_time": "2025-04-30T09:52:06.183680",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.040412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 22430\n",
      "Test dataset size: 4000\n",
      "Class distribution:\n",
      "Class  0: 1300 samples\n",
      "Class  1: 1300 samples\n",
      "Class  2: 1300 samples\n",
      "Class  3: 1300 samples\n",
      "Class  4: 1300 samples\n",
      "Class  5: 755 samples\n",
      "Class  6: 1300 samples\n",
      "Class  7: 658 samples\n",
      "Class  8: 1300 samples\n",
      "Class  9: 1300 samples\n",
      "Class 10: 1300 samples\n",
      "Class 11: 1300 samples\n",
      "Class 12: 1300 samples\n",
      "Class 13: 756 samples\n",
      "Class 14: 1300 samples\n",
      "Class 15: 550 samples\n",
      "Class 16: 751 samples\n",
      "Class 17: 1300 samples\n",
      "Class 18: 1300 samples\n",
      "Class 19: 760 samples\n"
     ]
    }
   ],
   "source": [
    "datasets_dir = '/kaggle/input/unipd-deep-learning-2025-challenge-1/'\n",
    "\n",
    "# create datasets\n",
    "train_dataset = ImageDataset(datasets_dir + 'train_dataset', test=False)\n",
    "test_dataset = ImageDataset(datasets_dir + 'test_dataset', test=True)\n",
    "\n",
    "train_dataset_for_val   = ImageDataset(datasets_dir + 'train_dataset', test=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "#Class distribution\n",
    "labels_path = osp.join(datasets_dir + 'train_dataset', 'labels.csv')\n",
    "label_counter = Counter()\n",
    "with open(labels_path, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        label = int(row['label'])\n",
    "        label_counter[label] += 1\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for label, count in sorted(label_counter.items()):\n",
    "    print(f\"Class {label:2d}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9397d8",
   "metadata": {
    "papermill": {
     "duration": 0.003105,
     "end_time": "2025-04-30T09:52:06.190172",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.187067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Data Split and Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303b9bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.197124Z",
     "iopub.status.busy": "2025-04-30T09:52:06.196931Z",
     "iopub.status.idle": "2025-04-30T09:52:06.203526Z",
     "shell.execute_reply": "2025-04-30T09:52:06.202838Z"
    },
    "papermill": {
     "duration": 0.0113,
     "end_time": "2025-04-30T09:52:06.204575",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.193275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transformations and data agumentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    #transforms.ColorJitter(0.3,0.3,0.3,0.1),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.RandomPerspective(0.2, p=0.5),\n",
    "    #transforms.GaussianBlur(3, (0.1,2.0)),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset.transform = train_transforms\n",
    "train_dataset_for_val.transform = val_transforms\n",
    "test_dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00b7464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.211758Z",
     "iopub.status.busy": "2025-04-30T09:52:06.211551Z",
     "iopub.status.idle": "2025-04-30T09:52:06.243523Z",
     "shell.execute_reply": "2025-04-30T09:52:06.242712Z"
    },
    "papermill": {
     "duration": 0.036983,
     "end_time": "2025-04-30T09:52:06.244731",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.207748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 16822\n",
      "Size validation set: 5608\n"
     ]
    }
   ],
   "source": [
    "#Divide set in train and val set\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.75) #0.75\n",
    "val_size = total_size - train_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_indices, val_indices = random_split(range(total_size), [train_size, val_size], generator=generator)\n",
    "\n",
    "train_set = Subset(train_dataset, train_indices)\n",
    "val_set   = Subset(train_dataset_for_val, val_indices)\n",
    "\n",
    "print(f\"Size training set: {len(train_set)}\")\n",
    "print(f\"Size validation set: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e55d20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.252422Z",
     "iopub.status.busy": "2025-04-30T09:52:06.251828Z",
     "iopub.status.idle": "2025-04-30T09:52:06.274901Z",
     "shell.execute_reply": "2025-04-30T09:52:06.274179Z"
    },
    "papermill": {
     "duration": 0.027879,
     "end_time": "2025-04-30T09:52:06.275984",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.248105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [ 982  964  972  958  977  558  984  490  997  996 1006  959  949  575\n",
      "  961  417  570  958  987  562]\n",
      "Class weights: [0.00101833 0.00103734 0.00102881 0.00104384 0.00102354 0.00179211\n",
      " 0.00101626 0.00204082 0.00100301 0.00100402 0.00099404 0.00104275\n",
      " 0.00105374 0.00173913 0.00104058 0.00239808 0.00175439 0.00104384\n",
      " 0.00101317 0.00177936]\n"
     ]
    }
   ],
   "source": [
    "subset_targets = [train_dataset.targets[i] for i in train_set.indices]\n",
    "class_counts = np.bincount(np.array(subset_targets))\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "# Compute weights: inverse of class frequency\n",
    "class_weights = 1. / class_counts\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "sample_weights = [class_weights[label] for label in subset_targets]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8022d909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:06.283583Z",
     "iopub.status.busy": "2025-04-30T09:52:06.283336Z",
     "iopub.status.idle": "2025-04-30T09:52:07.392158Z",
     "shell.execute_reply": "2025-04-30T09:52:07.391183Z"
    },
    "papermill": {
     "duration": 1.114077,
     "end_time": "2025-04-30T09:52:07.393559",
     "exception": false,
     "start_time": "2025-04-30T09:52:06.279482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: torch.Size([64, 3, 128, 128])\n",
      "Batch label shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Check the shape of a sample\n",
    "sample_image, sample_label = next(iter(train_loader))\n",
    "print(f\"Batch image shape: {sample_image.shape}\")\n",
    "print(f\"Batch label shape: {sample_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cce79",
   "metadata": {
    "papermill": {
     "duration": 0.003153,
     "end_time": "2025-04-30T09:52:07.400312",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.397159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2475bd6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.425932Z",
     "iopub.status.busy": "2025-04-30T09:52:07.425285Z",
     "iopub.status.idle": "2025-04-30T09:52:07.437808Z",
     "shell.execute_reply": "2025-04-30T09:52:07.437264Z"
    },
    "papermill": {
     "duration": 0.017382,
     "end_time": "2025-04-30T09:52:07.438835",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.421453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: Input: 3 x 64 x 64\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False), # 3 input channels, 64 filters\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 64x64 → 64x32x32\n",
    "\n",
    "            # Block 2: Input: 64 x 32 x 32\n",
    "            nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 32x32 → 128x16x16\n",
    "\n",
    "            # Block 3: Input: 128 x 16 x 16\n",
    "            nn.Conv2d(128, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 16x16 → 256x8x8\n",
    "\n",
    "            # Block 4: Input: 256x8x8\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2), #0.2\n",
    "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), # 8x8 → 512x8x8\n",
    "\n",
    "            # Block 5: 512×8×8 → 512×4×4  (nuevo bloque)\n",
    "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Bottleneck and pooling\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))  # 256x4x4 → 256x1x1\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4), #0.5\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5), #0.4\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df5e041f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.446600Z",
     "iopub.status.busy": "2025-04-30T09:52:07.446327Z",
     "iopub.status.idle": "2025-04-30T09:52:07.772720Z",
     "shell.execute_reply": "2025-04-30T09:52:07.772114Z"
    },
    "papermill": {
     "duration": 0.331806,
     "end_time": "2025-04-30T09:52:07.774080",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.442274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ImageClassifier(num_classes=20).to(device)\n",
    "\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.08, weight=class_weights_tensor) #0.05\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.003, #0.003\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=150,\n",
    "    pct_start=0.1, # 10% warmup\n",
    "    anneal_strategy='cos',\n",
    "    cycle_momentum=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ed930",
   "metadata": {
    "papermill": {
     "duration": 0.003343,
     "end_time": "2025-04-30T09:52:07.781227",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.777884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256077c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.788824Z",
     "iopub.status.busy": "2025-04-30T09:52:07.788568Z",
     "iopub.status.idle": "2025-04-30T09:52:07.792527Z",
     "shell.execute_reply": "2025-04-30T09:52:07.791797Z"
    },
    "papermill": {
     "duration": 0.009017,
     "end_time": "2025-04-30T09:52:07.793659",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.784642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    predicted_labels = y_pred.argmax(dim=1)\n",
    "    correct = (predicted_labels == y_true).sum().item()\n",
    "    accuracy = correct / y_true.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe5fe7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.801706Z",
     "iopub.status.busy": "2025-04-30T09:52:07.801149Z",
     "iopub.status.idle": "2025-04-30T09:52:07.806094Z",
     "shell.execute_reply": "2025-04-30T09:52:07.805605Z"
    },
    "papermill": {
     "duration": 0.009907,
     "end_time": "2025-04-30T09:52:07.807025",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.797118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train function\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Computer loss and accuracy\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d600cea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.814725Z",
     "iopub.status.busy": "2025-04-30T09:52:07.814542Z",
     "iopub.status.idle": "2025-04-30T09:52:07.819262Z",
     "shell.execute_reply": "2025-04-30T09:52:07.818636Z"
    },
    "papermill": {
     "duration": 0.009821,
     "end_time": "2025-04-30T09:52:07.820318",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.810497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evaluate function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Disable gradient\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "101ebf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T09:52:07.827744Z",
     "iopub.status.busy": "2025-04-30T09:52:07.827559Z",
     "iopub.status.idle": "2025-04-30T13:06:30.477454Z",
     "shell.execute_reply": "2025-04-30T13:06:30.476161Z"
    },
    "papermill": {
     "duration": 11662.666168,
     "end_time": "2025-04-30T13:06:30.489860",
     "exception": false,
     "start_time": "2025-04-30T09:52:07.823692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Train Loss: 3.0017  Train Acc: 0.0735\n",
      "Val Loss:   2.8977  Val Acc:   0.0774\n",
      "Best model saved at epoch 1\n",
      "\n",
      "Epoch 2/150\n",
      "Train Loss: 2.8441  Train Acc: 0.1049\n",
      "Val Loss:   2.7380  Val Acc:   0.1123\n",
      "Best model saved at epoch 2\n",
      "\n",
      "Epoch 3/150\n",
      "Train Loss: 2.7538  Train Acc: 0.1203\n",
      "Val Loss:   2.6889  Val Acc:   0.1261\n",
      "Best model saved at epoch 3\n",
      "\n",
      "Epoch 4/150\n",
      "Train Loss: 2.6788  Train Acc: 0.1367\n",
      "Val Loss:   2.5703  Val Acc:   0.1862\n",
      "Best model saved at epoch 4\n",
      "\n",
      "Epoch 5/150\n",
      "Train Loss: 2.5447  Train Acc: 0.1859\n",
      "Val Loss:   2.2984  Val Acc:   0.2564\n",
      "Best model saved at epoch 5\n",
      "\n",
      "Epoch 6/150\n",
      "Train Loss: 2.4091  Train Acc: 0.2157\n",
      "Val Loss:   2.2554  Val Acc:   0.2316\n",
      "Best model saved at epoch 6\n",
      "\n",
      "Epoch 7/150\n",
      "Train Loss: 2.3276  Train Acc: 0.2429\n",
      "Val Loss:   2.2172  Val Acc:   0.2853\n",
      "Best model saved at epoch 7\n",
      "\n",
      "Epoch 8/150\n",
      "Train Loss: 2.2765  Train Acc: 0.2560\n",
      "Val Loss:   2.1500  Val Acc:   0.3047\n",
      "Best model saved at epoch 8\n",
      "\n",
      "Epoch 9/150\n",
      "Train Loss: 2.2581  Train Acc: 0.2644\n",
      "Val Loss:   2.1411  Val Acc:   0.2976\n",
      "Best model saved at epoch 9\n",
      "\n",
      "Epoch 10/150\n",
      "Train Loss: 2.2037  Train Acc: 0.2847\n",
      "Val Loss:   2.1328  Val Acc:   0.3167\n",
      "Best model saved at epoch 10\n",
      "\n",
      "Epoch 11/150\n",
      "Train Loss: 2.1857  Train Acc: 0.2922\n",
      "Val Loss:   2.0206  Val Acc:   0.3627\n",
      "Best model saved at epoch 11\n",
      "\n",
      "Epoch 12/150\n",
      "Train Loss: 2.1499  Train Acc: 0.3064\n",
      "Val Loss:   2.2032  Val Acc:   0.3299\n",
      "Epoch 13/150\n",
      "Train Loss: 2.1304  Train Acc: 0.3244\n",
      "Val Loss:   1.9467  Val Acc:   0.3761\n",
      "Best model saved at epoch 13\n",
      "\n",
      "Epoch 14/150\n",
      "Train Loss: 2.1007  Train Acc: 0.3331\n",
      "Val Loss:   1.9299  Val Acc:   0.3930\n",
      "Best model saved at epoch 14\n",
      "\n",
      "Epoch 15/150\n",
      "Train Loss: 2.0691  Train Acc: 0.3501\n",
      "Val Loss:   1.9332  Val Acc:   0.4128\n",
      "Epoch 16/150\n",
      "Train Loss: 2.0265  Train Acc: 0.3575\n",
      "Val Loss:   1.8874  Val Acc:   0.4110\n",
      "Best model saved at epoch 16\n",
      "\n",
      "Epoch 17/150\n",
      "Train Loss: 2.0072  Train Acc: 0.3749\n",
      "Val Loss:   1.7705  Val Acc:   0.4636\n",
      "Best model saved at epoch 17\n",
      "\n",
      "Epoch 18/150\n",
      "Train Loss: 1.9706  Train Acc: 0.3888\n",
      "Val Loss:   1.7750  Val Acc:   0.4604\n",
      "Epoch 19/150\n",
      "Train Loss: 1.9483  Train Acc: 0.4014\n",
      "Val Loss:   1.7881  Val Acc:   0.4638\n",
      "Epoch 20/150\n",
      "Train Loss: 1.9019  Train Acc: 0.4196\n",
      "Val Loss:   1.6963  Val Acc:   0.5014\n",
      "Best model saved at epoch 20\n",
      "\n",
      "Epoch 21/150\n",
      "Train Loss: 1.8898  Train Acc: 0.4237\n",
      "Val Loss:   1.6851  Val Acc:   0.5027\n",
      "Best model saved at epoch 21\n",
      "\n",
      "Epoch 22/150\n",
      "Train Loss: 1.8725  Train Acc: 0.4323\n",
      "Val Loss:   1.6637  Val Acc:   0.5153\n",
      "Best model saved at epoch 22\n",
      "\n",
      "Epoch 23/150\n",
      "Train Loss: 1.8494  Train Acc: 0.4425\n",
      "Val Loss:   1.6633  Val Acc:   0.5201\n",
      "Best model saved at epoch 23\n",
      "\n",
      "Epoch 24/150\n",
      "Train Loss: 1.8186  Train Acc: 0.4608\n",
      "Val Loss:   1.5991  Val Acc:   0.5317\n",
      "Best model saved at epoch 24\n",
      "\n",
      "Epoch 25/150\n",
      "Train Loss: 1.7967  Train Acc: 0.4628\n",
      "Val Loss:   1.7113  Val Acc:   0.5371\n",
      "Epoch 26/150\n",
      "Train Loss: 1.7681  Train Acc: 0.4782\n",
      "Val Loss:   1.5587  Val Acc:   0.5747\n",
      "Best model saved at epoch 26\n",
      "\n",
      "Epoch 27/150\n",
      "Train Loss: 1.7582  Train Acc: 0.4807\n",
      "Val Loss:   1.5506  Val Acc:   0.5546\n",
      "Best model saved at epoch 27\n",
      "\n",
      "Epoch 28/150\n",
      "Train Loss: 1.7266  Train Acc: 0.4935\n",
      "Val Loss:   1.4974  Val Acc:   0.5908\n",
      "Best model saved at epoch 28\n",
      "\n",
      "Epoch 29/150\n",
      "Train Loss: 1.6955  Train Acc: 0.5109\n",
      "Val Loss:   1.4575  Val Acc:   0.6073\n",
      "Best model saved at epoch 29\n",
      "\n",
      "Epoch 30/150\n",
      "Train Loss: 1.6854  Train Acc: 0.5146\n",
      "Val Loss:   1.4450  Val Acc:   0.6107\n",
      "Best model saved at epoch 30\n",
      "\n",
      "Epoch 31/150\n",
      "Train Loss: 1.6430  Train Acc: 0.5310\n",
      "Val Loss:   1.4906  Val Acc:   0.5966\n",
      "Epoch 32/150\n",
      "Train Loss: 1.6426  Train Acc: 0.5344\n",
      "Val Loss:   1.4756  Val Acc:   0.6079\n",
      "Epoch 33/150\n",
      "Train Loss: 1.6331  Train Acc: 0.5374\n",
      "Val Loss:   1.4137  Val Acc:   0.6343\n",
      "Best model saved at epoch 33\n",
      "\n",
      "Epoch 34/150\n",
      "Train Loss: 1.6052  Train Acc: 0.5512\n",
      "Val Loss:   1.4283  Val Acc:   0.6220\n",
      "Epoch 35/150\n",
      "Train Loss: 1.5952  Train Acc: 0.5524\n",
      "Val Loss:   1.5246  Val Acc:   0.6056\n",
      "Epoch 36/150\n",
      "Train Loss: 1.5617  Train Acc: 0.5671\n",
      "Val Loss:   1.3730  Val Acc:   0.6435\n",
      "Best model saved at epoch 36\n",
      "\n",
      "Epoch 37/150\n",
      "Train Loss: 1.5473  Train Acc: 0.5710\n",
      "Val Loss:   1.3773  Val Acc:   0.6448\n",
      "Epoch 38/150\n",
      "Train Loss: 1.5309  Train Acc: 0.5873\n",
      "Val Loss:   1.3687  Val Acc:   0.6496\n",
      "Best model saved at epoch 38\n",
      "\n",
      "Epoch 39/150\n",
      "Train Loss: 1.5253  Train Acc: 0.5865\n",
      "Val Loss:   1.3206  Val Acc:   0.6633\n",
      "Best model saved at epoch 39\n",
      "\n",
      "Epoch 40/150\n",
      "Train Loss: 1.5157  Train Acc: 0.5870\n",
      "Val Loss:   1.3291  Val Acc:   0.6705\n",
      "Epoch 41/150\n",
      "Train Loss: 1.4830  Train Acc: 0.6035\n",
      "Val Loss:   1.3192  Val Acc:   0.6699\n",
      "Best model saved at epoch 41\n",
      "\n",
      "Epoch 42/150\n",
      "Train Loss: 1.4616  Train Acc: 0.6062\n",
      "Val Loss:   1.3070  Val Acc:   0.6803\n",
      "Best model saved at epoch 42\n",
      "\n",
      "Epoch 43/150\n",
      "Train Loss: 1.4650  Train Acc: 0.6105\n",
      "Val Loss:   1.2900  Val Acc:   0.6815\n",
      "Best model saved at epoch 43\n",
      "\n",
      "Epoch 44/150\n",
      "Train Loss: 1.4707  Train Acc: 0.6063\n",
      "Val Loss:   1.2930  Val Acc:   0.6776\n",
      "Epoch 45/150\n",
      "Train Loss: 1.4313  Train Acc: 0.6185\n",
      "Val Loss:   1.2817  Val Acc:   0.6840\n",
      "Best model saved at epoch 45\n",
      "\n",
      "Epoch 46/150\n",
      "Train Loss: 1.4336  Train Acc: 0.6214\n",
      "Val Loss:   1.2502  Val Acc:   0.6994\n",
      "Best model saved at epoch 46\n",
      "\n",
      "Epoch 47/150\n",
      "Train Loss: 1.4191  Train Acc: 0.6264\n",
      "Val Loss:   1.2616  Val Acc:   0.7002\n",
      "Epoch 48/150\n",
      "Train Loss: 1.3987  Train Acc: 0.6323\n",
      "Val Loss:   1.2452  Val Acc:   0.7054\n",
      "Best model saved at epoch 48\n",
      "\n",
      "Epoch 49/150\n",
      "Train Loss: 1.3888  Train Acc: 0.6427\n",
      "Val Loss:   1.2560  Val Acc:   0.7011\n",
      "Epoch 50/150\n",
      "Train Loss: 1.3685  Train Acc: 0.6421\n",
      "Val Loss:   1.2227  Val Acc:   0.7086\n",
      "Best model saved at epoch 50\n",
      "\n",
      "Epoch 51/150\n",
      "Train Loss: 1.3715  Train Acc: 0.6464\n",
      "Val Loss:   1.2274  Val Acc:   0.7093\n",
      "Epoch 52/150\n",
      "Train Loss: 1.3458  Train Acc: 0.6571\n",
      "Val Loss:   1.2253  Val Acc:   0.7077\n",
      "Epoch 53/150\n",
      "Train Loss: 1.3421  Train Acc: 0.6572\n",
      "Val Loss:   1.2072  Val Acc:   0.7150\n",
      "Best model saved at epoch 53\n",
      "\n",
      "Epoch 54/150\n",
      "Train Loss: 1.3310  Train Acc: 0.6673\n",
      "Val Loss:   1.1817  Val Acc:   0.7274\n",
      "Best model saved at epoch 54\n",
      "\n",
      "Epoch 55/150\n",
      "Train Loss: 1.3222  Train Acc: 0.6687\n",
      "Val Loss:   1.1986  Val Acc:   0.7170\n",
      "Epoch 56/150\n",
      "Train Loss: 1.3068  Train Acc: 0.6705\n",
      "Val Loss:   1.1717  Val Acc:   0.7388\n",
      "Best model saved at epoch 56\n",
      "\n",
      "Epoch 57/150\n",
      "Train Loss: 1.2972  Train Acc: 0.6792\n",
      "Val Loss:   1.1957  Val Acc:   0.7309\n",
      "Epoch 58/150\n",
      "Train Loss: 1.2761  Train Acc: 0.6862\n",
      "Val Loss:   1.1846  Val Acc:   0.7347\n",
      "Epoch 59/150\n",
      "Train Loss: 1.2695  Train Acc: 0.6876\n",
      "Val Loss:   1.1588  Val Acc:   0.7429\n",
      "Best model saved at epoch 59\n",
      "\n",
      "Epoch 60/150\n",
      "Train Loss: 1.2652  Train Acc: 0.6919\n",
      "Val Loss:   1.1974  Val Acc:   0.7240\n",
      "Epoch 61/150\n",
      "Train Loss: 1.2604  Train Acc: 0.6947\n",
      "Val Loss:   1.1766  Val Acc:   0.7382\n",
      "Epoch 62/150\n",
      "Train Loss: 1.2571  Train Acc: 0.6914\n",
      "Val Loss:   1.1632  Val Acc:   0.7373\n",
      "Epoch 63/150\n",
      "Train Loss: 1.2395  Train Acc: 0.6979\n",
      "Val Loss:   1.1727  Val Acc:   0.7388\n",
      "Epoch 64/150\n",
      "Train Loss: 1.2343  Train Acc: 0.7066\n",
      "Val Loss:   1.1569  Val Acc:   0.7512\n",
      "Best model saved at epoch 64\n",
      "\n",
      "Epoch 65/150\n",
      "Train Loss: 1.2219  Train Acc: 0.7075\n",
      "Val Loss:   1.1439  Val Acc:   0.7473\n",
      "Best model saved at epoch 65\n",
      "\n",
      "Epoch 66/150\n",
      "Train Loss: 1.2085  Train Acc: 0.7158\n",
      "Val Loss:   1.1547  Val Acc:   0.7423\n",
      "Epoch 67/150\n",
      "Train Loss: 1.2000  Train Acc: 0.7178\n",
      "Val Loss:   1.1599  Val Acc:   0.7457\n",
      "Epoch 68/150\n",
      "Train Loss: 1.2012  Train Acc: 0.7154\n",
      "Val Loss:   1.1301  Val Acc:   0.7561\n",
      "Best model saved at epoch 68\n",
      "\n",
      "Epoch 69/150\n",
      "Train Loss: 1.1767  Train Acc: 0.7273\n",
      "Val Loss:   1.1314  Val Acc:   0.7580\n",
      "Epoch 70/150\n",
      "Train Loss: 1.1773  Train Acc: 0.7268\n",
      "Val Loss:   1.1325  Val Acc:   0.7518\n",
      "Epoch 71/150\n",
      "Train Loss: 1.1831  Train Acc: 0.7274\n",
      "Val Loss:   1.1414  Val Acc:   0.7495\n",
      "Epoch 72/150\n",
      "Train Loss: 1.1580  Train Acc: 0.7336\n",
      "Val Loss:   1.1052  Val Acc:   0.7641\n",
      "Best model saved at epoch 72\n",
      "\n",
      "Epoch 73/150\n",
      "Train Loss: 1.1608  Train Acc: 0.7348\n",
      "Val Loss:   1.1236  Val Acc:   0.7596\n",
      "Epoch 74/150\n",
      "Train Loss: 1.1576  Train Acc: 0.7352\n",
      "Val Loss:   1.1115  Val Acc:   0.7655\n",
      "Epoch 75/150\n",
      "Train Loss: 1.1475  Train Acc: 0.7362\n",
      "Val Loss:   1.1068  Val Acc:   0.7618\n",
      "Epoch 76/150\n",
      "Train Loss: 1.1346  Train Acc: 0.7448\n",
      "Val Loss:   1.1087  Val Acc:   0.7721\n",
      "Epoch 77/150\n",
      "Train Loss: 1.1207  Train Acc: 0.7502\n",
      "Val Loss:   1.1044  Val Acc:   0.7653\n",
      "Best model saved at epoch 77\n",
      "\n",
      "Epoch 78/150\n",
      "Train Loss: 1.1015  Train Acc: 0.7550\n",
      "Val Loss:   1.0920  Val Acc:   0.7728\n",
      "Best model saved at epoch 78\n",
      "\n",
      "Epoch 79/150\n",
      "Train Loss: 1.1245  Train Acc: 0.7489\n",
      "Val Loss:   1.0769  Val Acc:   0.7760\n",
      "Best model saved at epoch 79\n",
      "\n",
      "Epoch 80/150\n",
      "Train Loss: 1.1146  Train Acc: 0.7545\n",
      "Val Loss:   1.0904  Val Acc:   0.7751\n",
      "Epoch 81/150\n",
      "Train Loss: 1.0992  Train Acc: 0.7603\n",
      "Val Loss:   1.0812  Val Acc:   0.7759\n",
      "Epoch 82/150\n",
      "Train Loss: 1.0838  Train Acc: 0.7639\n",
      "Val Loss:   1.0859  Val Acc:   0.7764\n",
      "Epoch 83/150\n",
      "Train Loss: 1.0836  Train Acc: 0.7682\n",
      "Val Loss:   1.0892  Val Acc:   0.7775\n",
      "Epoch 84/150\n",
      "Train Loss: 1.0735  Train Acc: 0.7674\n",
      "Val Loss:   1.0853  Val Acc:   0.7825\n",
      "Epoch 85/150\n",
      "Train Loss: 1.0836  Train Acc: 0.7646\n",
      "Val Loss:   1.0849  Val Acc:   0.7801\n",
      "Epoch 86/150\n",
      "Train Loss: 1.0823  Train Acc: 0.7671\n",
      "Val Loss:   1.0755  Val Acc:   0.7767\n",
      "Best model saved at epoch 86\n",
      "\n",
      "Epoch 87/150\n",
      "Train Loss: 1.0600  Train Acc: 0.7755\n",
      "Val Loss:   1.0785  Val Acc:   0.7819\n",
      "Epoch 88/150\n",
      "Train Loss: 1.0526  Train Acc: 0.7751\n",
      "Val Loss:   1.0735  Val Acc:   0.7839\n",
      "Best model saved at epoch 88\n",
      "\n",
      "Epoch 89/150\n",
      "Train Loss: 1.0447  Train Acc: 0.7788\n",
      "Val Loss:   1.0685  Val Acc:   0.7855\n",
      "Best model saved at epoch 89\n",
      "\n",
      "Epoch 90/150\n",
      "Train Loss: 1.0474  Train Acc: 0.7859\n",
      "Val Loss:   1.0700  Val Acc:   0.7835\n",
      "Epoch 91/150\n",
      "Train Loss: 1.0451  Train Acc: 0.7815\n",
      "Val Loss:   1.0634  Val Acc:   0.7876\n",
      "Best model saved at epoch 91\n",
      "\n",
      "Epoch 92/150\n",
      "Train Loss: 1.0453  Train Acc: 0.7834\n",
      "Val Loss:   1.0590  Val Acc:   0.7842\n",
      "Best model saved at epoch 92\n",
      "\n",
      "Epoch 93/150\n",
      "Train Loss: 1.0217  Train Acc: 0.7916\n",
      "Val Loss:   1.0609  Val Acc:   0.7885\n",
      "Epoch 94/150\n",
      "Train Loss: 1.0259  Train Acc: 0.7865\n",
      "Val Loss:   1.0589  Val Acc:   0.7874\n",
      "Best model saved at epoch 94\n",
      "\n",
      "Epoch 95/150\n",
      "Train Loss: 1.0251  Train Acc: 0.7881\n",
      "Val Loss:   1.0564  Val Acc:   0.7867\n",
      "Best model saved at epoch 95\n",
      "\n",
      "Epoch 96/150\n",
      "Train Loss: 1.0188  Train Acc: 0.7927\n",
      "Val Loss:   1.0498  Val Acc:   0.7889\n",
      "Best model saved at epoch 96\n",
      "\n",
      "Epoch 97/150\n",
      "Train Loss: 1.0271  Train Acc: 0.7916\n",
      "Val Loss:   1.0512  Val Acc:   0.7899\n",
      "Epoch 98/150\n",
      "Train Loss: 1.0008  Train Acc: 0.7991\n",
      "Val Loss:   1.0553  Val Acc:   0.7933\n",
      "Epoch 99/150\n",
      "Train Loss: 1.0061  Train Acc: 0.7960\n",
      "Val Loss:   1.0573  Val Acc:   0.7939\n",
      "Epoch 100/150\n",
      "Train Loss: 0.9881  Train Acc: 0.8054\n",
      "Val Loss:   1.0488  Val Acc:   0.7919\n",
      "Best model saved at epoch 100\n",
      "\n",
      "Epoch 101/150\n",
      "Train Loss: 0.9995  Train Acc: 0.8006\n",
      "Val Loss:   1.0515  Val Acc:   0.7905\n",
      "Epoch 102/150\n",
      "Train Loss: 0.9880  Train Acc: 0.8029\n",
      "Val Loss:   1.0484  Val Acc:   0.7907\n",
      "Best model saved at epoch 102\n",
      "\n",
      "Epoch 103/150\n",
      "Train Loss: 0.9877  Train Acc: 0.8042\n",
      "Val Loss:   1.0501  Val Acc:   0.7912\n",
      "Epoch 104/150\n",
      "Train Loss: 0.9897  Train Acc: 0.8072\n",
      "Val Loss:   1.0473  Val Acc:   0.7932\n",
      "Best model saved at epoch 104\n",
      "\n",
      "Epoch 105/150\n",
      "Train Loss: 0.9770  Train Acc: 0.8047\n",
      "Val Loss:   1.0411  Val Acc:   0.7981\n",
      "Best model saved at epoch 105\n",
      "\n",
      "Epoch 106/150\n",
      "Train Loss: 0.9775  Train Acc: 0.8101\n",
      "Val Loss:   1.0426  Val Acc:   0.7971\n",
      "Epoch 107/150\n",
      "Train Loss: 0.9688  Train Acc: 0.8128\n",
      "Val Loss:   1.0399  Val Acc:   0.8008\n",
      "Best model saved at epoch 107\n",
      "\n",
      "Epoch 108/150\n",
      "Train Loss: 0.9581  Train Acc: 0.8139\n",
      "Val Loss:   1.0478  Val Acc:   0.7958\n",
      "Epoch 109/150\n",
      "Train Loss: 0.9597  Train Acc: 0.8155\n",
      "Val Loss:   1.0424  Val Acc:   0.7946\n",
      "Epoch 110/150\n",
      "Train Loss: 0.9615  Train Acc: 0.8154\n",
      "Val Loss:   1.0427  Val Acc:   0.7912\n",
      "Epoch 111/150\n",
      "Train Loss: 0.9543  Train Acc: 0.8186\n",
      "Val Loss:   1.0414  Val Acc:   0.7956\n",
      "Epoch 112/150\n",
      "Train Loss: 0.9507  Train Acc: 0.8229\n",
      "Val Loss:   1.0462  Val Acc:   0.7903\n",
      "Epoch 113/150\n",
      "Train Loss: 0.9447  Train Acc: 0.8227\n",
      "Val Loss:   1.0510  Val Acc:   0.7937\n",
      "Epoch 114/150\n",
      "Train Loss: 0.9493  Train Acc: 0.8230\n",
      "Val Loss:   1.0446  Val Acc:   0.7939\n",
      "Epoch 115/150\n",
      "Train Loss: 0.9399  Train Acc: 0.8237\n",
      "Val Loss:   1.0423  Val Acc:   0.7953\n",
      "Epoch 116/150\n",
      "Train Loss: 0.9424  Train Acc: 0.8249\n",
      "Val Loss:   1.0369  Val Acc:   0.7942\n",
      "Best model saved at epoch 116\n",
      "\n",
      "Epoch 117/150\n",
      "Train Loss: 0.9383  Train Acc: 0.8286\n",
      "Val Loss:   1.0431  Val Acc:   0.7971\n",
      "Epoch 118/150\n",
      "Train Loss: 0.9474  Train Acc: 0.8237\n",
      "Val Loss:   1.0437  Val Acc:   0.7969\n",
      "Epoch 119/150\n",
      "Train Loss: 0.9340  Train Acc: 0.8316\n",
      "Val Loss:   1.0425  Val Acc:   0.7939\n",
      "Epoch 120/150\n",
      "Train Loss: 0.9350  Train Acc: 0.8327\n",
      "Val Loss:   1.0465  Val Acc:   0.7914\n",
      "Epoch 121/150\n",
      "Train Loss: 0.9353  Train Acc: 0.8284\n",
      "Val Loss:   1.0430  Val Acc:   0.7996\n",
      "Epoch 122/150\n",
      "Train Loss: 0.9362  Train Acc: 0.8264\n",
      "Val Loss:   1.0419  Val Acc:   0.7989\n",
      "Epoch 123/150\n",
      "Train Loss: 0.9134  Train Acc: 0.8379\n",
      "Val Loss:   1.0326  Val Acc:   0.8047\n",
      "Best model saved at epoch 123\n",
      "\n",
      "Epoch 124/150\n",
      "Train Loss: 0.9208  Train Acc: 0.8365\n",
      "Val Loss:   1.0353  Val Acc:   0.7989\n",
      "Epoch 125/150\n",
      "Train Loss: 0.9162  Train Acc: 0.8333\n",
      "Val Loss:   1.0351  Val Acc:   0.8012\n",
      "Epoch 126/150\n",
      "Train Loss: 0.9122  Train Acc: 0.8390\n",
      "Val Loss:   1.0321  Val Acc:   0.7983\n",
      "Best model saved at epoch 126\n",
      "\n",
      "Epoch 127/150\n",
      "Train Loss: 0.9099  Train Acc: 0.8386\n",
      "Val Loss:   1.0361  Val Acc:   0.8008\n",
      "Epoch 128/150\n",
      "Train Loss: 0.9201  Train Acc: 0.8388\n",
      "Val Loss:   1.0336  Val Acc:   0.8040\n",
      "Epoch 129/150\n",
      "Train Loss: 0.9086  Train Acc: 0.8394\n",
      "Val Loss:   1.0289  Val Acc:   0.8006\n",
      "Best model saved at epoch 129\n",
      "\n",
      "Epoch 130/150\n",
      "Train Loss: 0.9178  Train Acc: 0.8377\n",
      "Val Loss:   1.0302  Val Acc:   0.8003\n",
      "Epoch 131/150\n",
      "Train Loss: 0.9097  Train Acc: 0.8356\n",
      "Val Loss:   1.0323  Val Acc:   0.7981\n",
      "Epoch 132/150\n",
      "Train Loss: 0.9136  Train Acc: 0.8355\n",
      "Val Loss:   1.0290  Val Acc:   0.8008\n",
      "Epoch 133/150\n",
      "Train Loss: 0.8992  Train Acc: 0.8459\n",
      "Val Loss:   1.0294  Val Acc:   0.8017\n",
      "Epoch 134/150\n",
      "Train Loss: 0.9026  Train Acc: 0.8455\n",
      "Val Loss:   1.0318  Val Acc:   0.7999\n",
      "Epoch 135/150\n",
      "Train Loss: 0.9071  Train Acc: 0.8440\n",
      "Val Loss:   1.0314  Val Acc:   0.8022\n",
      "Epoch 136/150\n",
      "Train Loss: 0.9076  Train Acc: 0.8410\n",
      "Val Loss:   1.0290  Val Acc:   0.8012\n",
      "Epoch 137/150\n",
      "Train Loss: 0.8986  Train Acc: 0.8453\n",
      "Val Loss:   1.0341  Val Acc:   0.7996\n",
      "Epoch 138/150\n",
      "Train Loss: 0.9168  Train Acc: 0.8348\n",
      "Val Loss:   1.0328  Val Acc:   0.8024\n",
      "Epoch 139/150\n",
      "Train Loss: 0.8958  Train Acc: 0.8448\n",
      "Val Loss:   1.0285  Val Acc:   0.8031\n",
      "Best model saved at epoch 139\n",
      "\n",
      "Epoch 140/150\n",
      "Train Loss: 0.8942  Train Acc: 0.8460\n",
      "Val Loss:   1.0264  Val Acc:   0.8017\n",
      "Best model saved at epoch 140\n",
      "\n",
      "Epoch 141/150\n",
      "Train Loss: 0.9116  Train Acc: 0.8391\n",
      "Val Loss:   1.0272  Val Acc:   0.8040\n",
      "Epoch 142/150\n",
      "Train Loss: 0.9138  Train Acc: 0.8397\n",
      "Val Loss:   1.0229  Val Acc:   0.8044\n",
      "Best model saved at epoch 142\n",
      "\n",
      "Epoch 143/150\n",
      "Train Loss: 0.9078  Train Acc: 0.8415\n",
      "Val Loss:   1.0276  Val Acc:   0.8035\n",
      "Epoch 144/150\n",
      "Train Loss: 0.9103  Train Acc: 0.8401\n",
      "Val Loss:   1.0319  Val Acc:   0.8010\n",
      "Epoch 145/150\n",
      "Train Loss: 0.8986  Train Acc: 0.8458\n",
      "Val Loss:   1.0296  Val Acc:   0.8031\n",
      "Epoch 146/150\n",
      "Train Loss: 0.8965  Train Acc: 0.8438\n",
      "Val Loss:   1.0290  Val Acc:   0.8037\n",
      "Epoch 147/150\n",
      "Train Loss: 0.9102  Train Acc: 0.8430\n",
      "Val Loss:   1.0341  Val Acc:   0.8003\n",
      "Epoch 148/150\n",
      "Train Loss: 0.8987  Train Acc: 0.8440\n",
      "Val Loss:   1.0259  Val Acc:   0.8022\n",
      "Epoch 149/150\n",
      "Train Loss: 0.9053  Train Acc: 0.8403\n",
      "Val Loss:   1.0279  Val Acc:   0.8037\n",
      "Epoch 150/150\n",
      "Train Loss: 0.8964  Train Acc: 0.8457\n",
      "Val Loss:   1.0313  Val Acc:   0.8031\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Print results by epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}  Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "    #Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Best model saved at epoch {epoch + 1}\\n\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3ad2a",
   "metadata": {
    "papermill": {
     "duration": 0.010499,
     "end_time": "2025-04-30T13:06:30.511088",
     "exception": false,
     "start_time": "2025-04-30T13:06:30.500589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Test Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b917e775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:06:30.534510Z",
     "iopub.status.busy": "2025-04-30T13:06:30.534157Z",
     "iopub.status.idle": "2025-04-30T13:06:40.247549Z",
     "shell.execute_reply": "2025-04-30T13:06:40.246346Z"
    },
    "papermill": {
     "duration": 9.72701,
     "end_time": "2025-04-30T13:06:40.249270",
     "exception": false,
     "start_time": "2025-04-30T13:06:30.522260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/565663662.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for images, img_ids in test_loader:\n",
    "      images = images.to(device)\n",
    "      preds = model(images)\n",
    "      predicted_labels = preds.argmax(dim=1).cpu().numpy()\n",
    "      results.extend(zip(img_ids, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eccd5133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:06:40.273719Z",
     "iopub.status.busy": "2025-04-30T13:06:40.273355Z",
     "iopub.status.idle": "2025-04-30T13:06:40.317867Z",
     "shell.execute_reply": "2025-04-30T13:06:40.316969Z"
    },
    "papermill": {
     "duration": 0.058439,
     "end_time": "2025-04-30T13:06:40.319451",
     "exception": false,
     "start_time": "2025-04-30T13:06:40.261012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['id', 'label'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11751646,
     "sourceId": 96834,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11693.876231,
   "end_time": "2025-04-30T13:06:43.636747",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-30T09:51:49.760516",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
